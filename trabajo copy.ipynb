{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96a46c6b",
   "metadata": {},
   "source": [
    "# üçé FruitSeg30 - Clasificaci√≥n de Frutas con Deep Learning\n",
    "\n",
    "## Proyecto de Clasificaci√≥n y B√∫squeda de Similaridad\n",
    "\n",
    "Este notebook implementa m√∫ltiples arquitecturas de Deep Learning para clasificaci√≥n de 30 tipos de frutas:\n",
    "\n",
    "1. **CNN desde cero** - Arquitectura convolucional dise√±ada manualmente\n",
    "2. **ViT/Swin Transformer** - Transfer learning con transformers visuales\n",
    "3. **Red Siamesa + Contrastive Loss** ‚Üí Clasificaci√≥n con FC layer\n",
    "4. **Red Siamesa + Contrastive Loss** ‚Üí Clasificaci√≥n con XGBoost\n",
    "5. **Red Siamesa + Triplet Loss** ‚Üí Clasificaci√≥n con FC layer\n",
    "6. **Red Siamesa + Triplet Loss** ‚Üí Clasificaci√≥n con XGBoost\n",
    "7. **Buscador de Similaridad** - Top 10 im√°genes similares\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16afbad2",
   "metadata": {},
   "source": [
    "## 1. Setup e Importaci√≥n de Librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6595869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librer√≠as b√°sicas\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualizaci√≥n\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import swin_t, Swin_T_Weights\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è XGBoost no disponible. Se omitir√°n las secciones con XGBoost.\")\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configuraci√≥n del dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Dispositivo: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Semilla para reproducibilidad\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "print(\"‚úÖ Semilla establecida: 42\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e346bbf",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n y Carga de Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuraci√≥n del proyecto\n",
    "class Config:\n",
    "    # Rutas\n",
    "    DATA_DIR = Path(\"data\")\n",
    "    \n",
    "    # Par√°metros de imagen\n",
    "    IMG_SIZE = 224  # Tama√±o para modelos preentrenados\n",
    "    ORIGINAL_SIZE = 512  # Tama√±o original del dataset\n",
    "    \n",
    "    # Par√°metros de entrenamiento\n",
    "    BATCH_SIZE = 32\n",
    "    NUM_WORKERS = 0\n",
    "    NUM_EPOCHS_CNN = 50\n",
    "    NUM_EPOCHS_SWIN = 30\n",
    "    NUM_EPOCHS_SIAMESE = 50\n",
    "    \n",
    "    # Learning rates\n",
    "    LR_CNN = 1e-3\n",
    "    LR_SWIN = 1e-4\n",
    "    LR_SIAMESE = 1e-4\n",
    "    \n",
    "    # Split de datos\n",
    "    TRAIN_RATIO = 0.70\n",
    "    VAL_RATIO = 0.15\n",
    "    TEST_RATIO = 0.15\n",
    "    \n",
    "    # N√∫mero de clases\n",
    "    NUM_CLASSES = 30\n",
    "    \n",
    "    # Embedding dimension para redes siamesas\n",
    "    EMBEDDING_DIM = 128\n",
    "\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuraci√≥n cargada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9affd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorar el dataset\n",
    "def explore_dataset(data_dir):\n",
    "    \"\"\"Explora el dataset y devuelve informaci√≥n sobre las clases\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    \n",
    "    class_info = []\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for class_folder in sorted(data_dir.iterdir()):\n",
    "        if class_folder.is_dir():\n",
    "            images_folder = class_folder / \"Images\"\n",
    "            if images_folder.exists():\n",
    "                images = list(images_folder.glob(\"*.jpg\")) + list(images_folder.glob(\"*.JPG\"))\n",
    "                class_name = class_folder.name\n",
    "                \n",
    "                for img_path in images:\n",
    "                    all_images.append(str(img_path))\n",
    "                    all_labels.append(class_name)\n",
    "                \n",
    "                class_info.append({\n",
    "                    'class': class_name,\n",
    "                    'num_images': len(images)\n",
    "                })\n",
    "    \n",
    "    df_classes = pd.DataFrame(class_info)\n",
    "    return df_classes, all_images, all_labels\n",
    "\n",
    "# Explorar dataset\n",
    "df_classes, all_images, all_labels = explore_dataset(config.DATA_DIR)\n",
    "\n",
    "print(f\"üìä Estad√≠sticas del Dataset FruitSeg30:\")\n",
    "print(f\"   ‚Ä¢ Total de clases: {len(df_classes)}\")\n",
    "print(f\"   ‚Ä¢ Total de im√°genes: {len(all_images)}\")\n",
    "print(f\"   ‚Ä¢ Im√°genes por clase (promedio): {len(all_images)/len(df_classes):.1f}\")\n",
    "print(f\"\\nüìà Distribuci√≥n de im√°genes por clase:\")\n",
    "print(df_classes.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b720cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar distribuci√≥n de clases\n",
    "plt.figure(figsize=(14, 6))\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, len(df_classes)))\n",
    "bars = plt.bar(range(len(df_classes)), df_classes['num_images'], color=colors)\n",
    "plt.xticks(range(len(df_classes)), df_classes['class'], rotation=45, ha='right')\n",
    "plt.xlabel('Clase de Fruta')\n",
    "plt.ylabel('N√∫mero de Im√°genes')\n",
    "plt.title('üìä Distribuci√≥n de Im√°genes por Clase - FruitSeg30')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualizar algunas im√°genes de ejemplo\n",
    "fig, axes = plt.subplots(3, 10, figsize=(20, 6))\n",
    "sampled_classes = random.sample(list(df_classes['class']), min(10, len(df_classes)))\n",
    "\n",
    "for idx, class_name in enumerate(sampled_classes):\n",
    "    # Obtener im√°genes de esta clase\n",
    "    class_images = [img for img, lbl in zip(all_images, all_labels) if lbl == class_name]\n",
    "    \n",
    "    for row in range(3):\n",
    "        if row < len(class_images):\n",
    "            img = Image.open(class_images[row])\n",
    "            axes[row, idx].imshow(img)\n",
    "        axes[row, idx].axis('off')\n",
    "        if row == 0:\n",
    "            axes[row, idx].set_title(class_name[:12], fontsize=8)\n",
    "\n",
    "plt.suptitle('üçé Muestras del Dataset FruitSeg30', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9c7237",
   "metadata": {},
   "source": [
    "## 3. Preparaci√≥n de Datos y Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d053874f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformaciones para entrenamiento y validaci√≥n/test\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.3),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_test_transforms = transforms.Compose([\n",
    "    transforms.Resize((config.IMG_SIZE, config.IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "print(\"‚úÖ Transformaciones definidas\")\n",
    "print(f\"   ‚Ä¢ Train: Resize, Flips, Rotaci√≥n, ColorJitter, Affine, Normalize\")\n",
    "print(f\"   ‚Ä¢ Val/Test: Resize, Normalize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d213bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset personalizado para clasificaci√≥n\n",
    "class FruitDataset(Dataset):\n",
    "    \"\"\"Dataset para clasificaci√≥n de frutas\"\"\"\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Crear codificador de etiquetas\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.encoded_labels = self.label_encoder.fit_transform(labels)\n",
    "        self.num_classes = len(self.label_encoder.classes_)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        label = self.encoded_labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    def get_class_names(self):\n",
    "        return self.label_encoder.classes_\n",
    "\n",
    "# Crear dataset completo para obtener el label encoder\n",
    "full_dataset = FruitDataset(all_images, all_labels, transform=None)\n",
    "label_encoder = full_dataset.label_encoder\n",
    "class_names = full_dataset.get_class_names()\n",
    "\n",
    "print(f\"‚úÖ Dataset creado\")\n",
    "print(f\"   ‚Ä¢ Total im√°genes: {len(full_dataset)}\")\n",
    "print(f\"   ‚Ä¢ Clases: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b2f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split estratificado: 70% train, 15% validation, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    all_images, all_labels, \n",
    "    test_size=0.30, \n",
    "    stratify=all_labels, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, \n",
    "    test_size=0.50, \n",
    "    stratify=y_temp, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"üìä Split de datos:\")\n",
    "print(f\"   ‚Ä¢ Train: {len(X_train)} im√°genes ({len(X_train)/len(all_images)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Validation: {len(X_val)} im√°genes ({len(X_val)/len(all_images)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Test: {len(X_test)} im√°genes ({len(X_test)/len(all_images)*100:.1f}%)\")\n",
    "\n",
    "# Verificar distribuci√≥n de clases en cada split\n",
    "def check_distribution(labels, name):\n",
    "    counter = Counter(labels)\n",
    "    print(f\"\\n   {name}: min={min(counter.values())}, max={max(counter.values())}, media={np.mean(list(counter.values())):.1f}\")\n",
    "\n",
    "check_distribution(y_train, \"Train\")\n",
    "check_distribution(y_val, \"Val\")\n",
    "check_distribution(y_test, \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da0e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear datasets con transformaciones\n",
    "train_dataset = FruitDataset(X_train, y_train, transform=train_transforms)\n",
    "val_dataset = FruitDataset(X_val, y_val, transform=val_test_transforms)\n",
    "test_dataset = FruitDataset(X_test, y_test, transform=val_test_transforms)\n",
    "\n",
    "# Crear dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ DataLoaders creados\")\n",
    "print(f\"   ‚Ä¢ Train batches: {len(train_loader)}\")\n",
    "print(f\"   ‚Ä¢ Val batches: {len(val_loader)}\")\n",
    "print(f\"   ‚Ä¢ Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d63e4",
   "metadata": {},
   "source": [
    "## 4. Funciones de Entrenamiento y Evaluaci√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1cd976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de entrenamiento y evaluaci√≥n\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Entrena el modelo por una √©poca\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for images, labels in pbar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'acc': f'{100.*correct/total:.2f}%'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Valida el modelo\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels).sum().item()\n",
    "    \n",
    "    epoch_loss = running_loss / len(val_loader)\n",
    "    epoch_acc = 100. * correct / total\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def evaluate_model(model, test_loader, device, class_names):\n",
    "    \"\"\"Eval√∫a el modelo en el conjunto de test\"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nüìä Accuracy en Test: {accuracy*100:.2f}%\")\n",
    "    \n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "    \n",
    "    return all_labels, all_preds\n",
    "\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, title=\"Training History\"):\n",
    "    \"\"\"Visualiza el historial de entrenamiento\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Loss\n",
    "    ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "    ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "    ax1.set_xlabel('√âpoca')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title(f'{title} - Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Accuracy\n",
    "    ax2.plot(train_accs, label='Train Acc', color='blue')\n",
    "    ax2.plot(val_accs, label='Val Acc', color='red')\n",
    "    ax2.set_xlabel('√âpoca')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.set_title(f'{title} - Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, title=\"Confusion Matrix\"):\n",
    "    \"\"\"Visualiza la matriz de confusi√≥n\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicci√≥n')\n",
    "    plt.ylabel('Real')\n",
    "    plt.title(title)\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"‚úÖ Funciones de entrenamiento y evaluaci√≥n definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c3d3f7",
   "metadata": {},
   "source": [
    "## 5. CNN desde Cero - Clasificaci√≥n\n",
    "\n",
    "Arquitectura convolucional dise√±ada manualmente con:\n",
    "- Bloques convolucionales con BatchNorm y MaxPooling\n",
    "- Global Average Pooling\n",
    "- Capas fully connected con Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73f391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arquitectura CNN desde cero (Simplificada para evitar overfitting)\n",
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"Bloque convolucional con BatchNorm y ReLU\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.relu(self.bn(self.conv(x)))\n",
    "\n",
    "class FruitCNN(nn.Module):\n",
    "    \"\"\"CNN simplificada para clasificaci√≥n de frutas - menos par√°metros para evitar overfitting\"\"\"\n",
    "    def __init__(self, num_classes=30, dropout_rate=0.5):\n",
    "        super(FruitCNN, self).__init__()\n",
    "        \n",
    "        # Bloque 1: 3 -> 32 canales (reducido de 64)\n",
    "        self.block1 = nn.Sequential(\n",
    "            ConvBlock(3, 32),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.MaxPool2d(2, 2)  # 224 -> 112\n",
    "        )\n",
    "        \n",
    "        # Bloque 2: 32 -> 64 canales (reducido de 128)\n",
    "        self.block2 = nn.Sequential(\n",
    "            ConvBlock(32, 64),\n",
    "            nn.Dropout2d(0.1),\n",
    "            nn.MaxPool2d(2, 2)  # 112 -> 56\n",
    "        )\n",
    "        \n",
    "        # Bloque 3: 64 -> 128 canales (reducido de 256)\n",
    "        self.block3 = nn.Sequential(\n",
    "            ConvBlock(64, 128),\n",
    "            ConvBlock(128, 128),\n",
    "            nn.Dropout2d(0.2),\n",
    "            nn.MaxPool2d(2, 2)  # 56 -> 28\n",
    "        )\n",
    "        \n",
    "        # Bloque 4: 128 -> 256 canales (reducido de 512)\n",
    "        self.block4 = nn.Sequential(\n",
    "            ConvBlock(128, 256),\n",
    "            ConvBlock(256, 256),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.MaxPool2d(2, 2)  # 28 -> 14\n",
    "        )\n",
    "        \n",
    "        # Bloque 5: 256 -> 256 canales (reducido de 512)\n",
    "        self.block5 = nn.Sequential(\n",
    "            ConvBlock(256, 256),\n",
    "            nn.Dropout2d(0.3),\n",
    "            nn.MaxPool2d(2, 2)  # 14 -> 7\n",
    "        )\n",
    "        \n",
    "        # Global Average Pooling\n",
    "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
    "        \n",
    "        # Clasificador m√°s simple\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Inicializaci√≥n de pesos\n",
    "        self._initialize_weights()\n",
    "        \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.gap(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Crear modelo\n",
    "cnn_model = FruitCNN(num_classes=config.NUM_CLASSES, dropout_rate=0.5).to(device)\n",
    "\n",
    "# Mostrar arquitectura\n",
    "print(\"üèóÔ∏è Arquitectura CNN desde cero (Simplificada):\")\n",
    "print(f\"   ‚Ä¢ Par√°metros totales: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "print(f\"   ‚Ä¢ Par√°metros entrenables: {sum(p.numel() for p in cnn_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b6c627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento de la CNN desde cero\n",
    "def train_cnn_model(model, train_loader, val_loader, num_epochs, lr, device, model_name=\"CNN\"):\n",
    "    \"\"\"Funci√≥n completa de entrenamiento\"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)  # M√°s weight decay\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"\\nüöÄ Iniciando entrenamiento de {model_name}\")\n",
    "    print(f\"   ‚Ä¢ √âpocas: {num_epochs}\")\n",
    "    print(f\"   ‚Ä¢ Learning Rate: {lr}\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Epoch':^8} | {'Train Loss':^12} | {'Train Acc':^12} | {'Val Loss':^12} | {'Val Acc':^12} | {'Best':^6}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        train_accs.append(train_acc)\n",
    "        val_accs.append(val_acc)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        # Guardar mejor modelo\n",
    "        is_best = \"\"\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            is_best = \"‚úì\"\n",
    "        \n",
    "        # Mostrar TODAS las √©pocas\n",
    "        print(f\"{epoch+1:^8} | {train_loss:^12.4f} | {train_acc:^11.2f}% | {val_loss:^12.4f} | {val_acc:^11.2f}% | {is_best:^6}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Cargar mejor modelo\n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n‚úÖ Mejor Val Accuracy: {best_val_acc:.2f}%\")\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Entrenar CNN\n",
    "cnn_model, cnn_train_losses, cnn_val_losses, cnn_train_accs, cnn_val_accs = train_cnn_model(\n",
    "    cnn_model, train_loader, val_loader, \n",
    "    num_epochs=config.NUM_EPOCHS_CNN, \n",
    "    lr=config.LR_CNN, \n",
    "    device=device,\n",
    "    model_name=\"CNN desde cero\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72510f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar historial de entrenamiento CNN\n",
    "plot_training_history(cnn_train_losses, cnn_val_losses, cnn_train_accs, cnn_val_accs, \n",
    "                      title=\"CNN desde cero\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6561531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar CNN en test\n",
    "print(\"üìä Evaluaci√≥n de CNN desde cero en conjunto de Test:\")\n",
    "cnn_labels, cnn_preds = evaluate_model(cnn_model, test_loader, device, class_names)\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "plot_confusion_matrix(cnn_labels, cnn_preds, class_names, title=\"CNN desde cero - Matriz de Confusi√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80069a5",
   "metadata": {},
   "source": [
    "## 6. Swin Transformer - Transfer Learning\n",
    "\n",
    "Utilizamos Swin Transformer Tiny preentrenado en ImageNet:\n",
    "- Fine-tuning de todas las capas\n",
    "- Cabeza de clasificaci√≥n personalizada para 30 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0096adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo Swin Transformer con Transfer Learning\n",
    "class SwinClassifier(nn.Module):\n",
    "    \"\"\"Swin Transformer para clasificaci√≥n de frutas\"\"\"\n",
    "    def __init__(self, num_classes=30, pretrained=True):\n",
    "        super(SwinClassifier, self).__init__()\n",
    "        \n",
    "        # Cargar Swin Transformer preentrenado\n",
    "        if pretrained:\n",
    "            self.swin = swin_t(weights=Swin_T_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.swin = swin_t(weights=None)\n",
    "        \n",
    "        # Obtener dimensi√≥n de features\n",
    "        in_features = self.swin.head.in_features\n",
    "        \n",
    "        # Reemplazar cabeza de clasificaci√≥n\n",
    "        self.swin.head = nn.Sequential(\n",
    "            nn.Linear(in_features, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.swin(x)\n",
    "\n",
    "# Crear modelo Swin\n",
    "swin_model = SwinClassifier(num_classes=config.NUM_CLASSES, pretrained=True).to(device)\n",
    "\n",
    "print(\"üèóÔ∏è Arquitectura Swin Transformer:\")\n",
    "print(f\"   ‚Ä¢ Par√°metros totales: {sum(p.numel() for p in swin_model.parameters()):,}\")\n",
    "print(f\"   ‚Ä¢ Par√°metros entrenables: {sum(p.numel() for p in swin_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3766efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar Swin Transformer\n",
    "swin_model, swin_train_losses, swin_val_losses, swin_train_accs, swin_val_accs = train_cnn_model(\n",
    "    swin_model, train_loader, val_loader, \n",
    "    num_epochs=config.NUM_EPOCHS_SWIN, \n",
    "    lr=config.LR_SWIN, \n",
    "    device=device,\n",
    "    model_name=\"Swin Transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c25005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar historial de entrenamiento Swin\n",
    "plot_training_history(swin_train_losses, swin_val_losses, swin_train_accs, swin_val_accs, \n",
    "                      title=\"Swin Transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c4542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar Swin en test\n",
    "print(\"üìä Evaluaci√≥n de Swin Transformer en conjunto de Test:\")\n",
    "swin_labels, swin_preds = evaluate_model(swin_model, test_loader, device, class_names)\n",
    "\n",
    "# Matriz de confusi√≥n\n",
    "plot_confusion_matrix(swin_labels, swin_preds, class_names, title=\"Swin Transformer - Matriz de Confusi√≥n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688aeba",
   "metadata": {},
   "source": [
    "## 7. Red Siamesa + Contrastive Loss\n",
    "\n",
    "Implementaci√≥n de una red siamesa para aprendizaje de similaridad:\n",
    "- Backbone: ResNet18 preentrenado\n",
    "- Contrastive Loss para aprender embeddings\n",
    "- Clasificaci√≥n mediante FC layer y XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054ac0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset para pares de im√°genes (Siamese)\n",
    "class SiameseDataset(Dataset):\n",
    "    \"\"\"Dataset que genera pares de im√°genes para red siamesa\"\"\"\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Organizar im√°genes por clase\n",
    "        self.class_to_indices = {}\n",
    "        for idx, label in enumerate(labels):\n",
    "            if label not in self.class_to_indices:\n",
    "                self.class_to_indices[label] = []\n",
    "            self.class_to_indices[label].append(idx)\n",
    "        \n",
    "        self.classes = list(self.class_to_indices.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1_path = self.image_paths[idx]\n",
    "        label1 = self.labels[idx]\n",
    "        \n",
    "        # 50% pares positivos (misma clase), 50% pares negativos\n",
    "        if random.random() > 0.5:\n",
    "            # Par positivo\n",
    "            idx2 = random.choice(self.class_to_indices[label1])\n",
    "            same_class = 1\n",
    "        else:\n",
    "            # Par negativo\n",
    "            other_classes = [c for c in self.classes if c != label1]\n",
    "            other_class = random.choice(other_classes)\n",
    "            idx2 = random.choice(self.class_to_indices[other_class])\n",
    "            same_class = 0\n",
    "        \n",
    "        img2_path = self.image_paths[idx2]\n",
    "        \n",
    "        img1 = Image.open(img1_path).convert('RGB')\n",
    "        img2 = Image.open(img2_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        \n",
    "        return img1, img2, torch.tensor(same_class, dtype=torch.float32)\n",
    "\n",
    "# Crear datasets siameses\n",
    "siamese_train_dataset = SiameseDataset(X_train, y_train, transform=train_transforms)\n",
    "siamese_val_dataset = SiameseDataset(X_val, y_val, transform=val_test_transforms)\n",
    "\n",
    "siamese_train_loader = DataLoader(siamese_train_dataset, batch_size=config.BATCH_SIZE, \n",
    "                                   shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "siamese_val_loader = DataLoader(siamese_val_dataset, batch_size=config.BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ Datasets siameses creados\")\n",
    "print(f\"   ‚Ä¢ Train pairs: {len(siamese_train_dataset)}\")\n",
    "print(f\"   ‚Ä¢ Val pairs: {len(siamese_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b321ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Siamesa con Contrastive Loss\n",
    "class SiameseNetwork(nn.Module):\n",
    "    \"\"\"Red Siamesa con backbone ResNet18\"\"\"\n",
    "    def __init__(self, embedding_dim=128):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        \n",
    "        # Backbone: ResNet18 preentrenado\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # Sin la √∫ltima capa FC\n",
    "        \n",
    "        # Embedding layer\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, embedding_dim)\n",
    "        )\n",
    "        \n",
    "    def forward_one(self, x):\n",
    "        \"\"\"Forward pass para una imagen\"\"\"\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.embedding(features)\n",
    "        return embedding\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"Forward pass para un par de im√°genes\"\"\"\n",
    "        embedding1 = self.forward_one(x1)\n",
    "        embedding2 = self.forward_one(x2)\n",
    "        return embedding1, embedding2\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"Contrastive Loss para redes siamesas\"\"\"\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, embedding1, embedding2, label):\n",
    "        # Distancia euclidiana\n",
    "        distance = F.pairwise_distance(embedding1, embedding2)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        # label = 1 para pares similares, 0 para pares diferentes\n",
    "        loss = label * distance.pow(2) + \\\n",
    "               (1 - label) * F.relu(self.margin - distance).pow(2)\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "# Crear red siamesa con contrastive loss\n",
    "siamese_contrastive = SiameseNetwork(embedding_dim=config.EMBEDDING_DIM).to(device)\n",
    "contrastive_loss_fn = ContrastiveLoss(margin=2.0)\n",
    "\n",
    "print(\"üèóÔ∏è Red Siamesa con Contrastive Loss:\")\n",
    "print(f\"   ‚Ä¢ Par√°metros totales: {sum(p.numel() for p in siamese_contrastive.parameters()):,}\")\n",
    "print(f\"   ‚Ä¢ Embedding dimension: {config.EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6267fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de entrenamiento para redes siamesas\n",
    "def train_siamese_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Entrena la red siamesa por una √©poca\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training Siamese\", leave=False)\n",
    "    for img1, img2, label in pbar:\n",
    "        img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        emb1, emb2 = model(img1, img2)\n",
    "        loss = criterion(emb1, emb2, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def validate_siamese(model, val_loader, criterion, device):\n",
    "    \"\"\"Valida la red siamesa\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img1, img2, label in val_loader:\n",
    "            img1, img2, label = img1.to(device), img2.to(device), label.to(device)\n",
    "            emb1, emb2 = model(img1, img2)\n",
    "            loss = criterion(emb1, emb2, label)\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(val_loader)\n",
    "\n",
    "def train_siamese_model(model, train_loader, val_loader, criterion, num_epochs, lr, device, model_name=\"Siamese\"):\n",
    "    \"\"\"Funci√≥n completa de entrenamiento de red siamesa\"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"\\nüöÄ Iniciando entrenamiento de {model_name}\")\n",
    "    print(f\"   ‚Ä¢ √âpocas: {num_epochs}\")\n",
    "    print(f\"   ‚Ä¢ Learning Rate: {lr}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_siamese_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate_siamese(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n‚úÖ Mejor Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "print(\"‚úÖ Funciones de entrenamiento siamesas definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8e614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar red siamesa con Contrastive Loss\n",
    "siamese_contrastive, siamese_c_train_losses, siamese_c_val_losses = train_siamese_model(\n",
    "    siamese_contrastive, siamese_train_loader, siamese_val_loader, \n",
    "    contrastive_loss_fn, num_epochs=config.NUM_EPOCHS_SIAMESE, \n",
    "    lr=config.LR_SIAMESE, device=device,\n",
    "    model_name=\"Red Siamesa (Contrastive Loss)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a5aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar p√©rdida de entrenamiento siamesa\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(siamese_c_train_losses, label='Train Loss', color='blue')\n",
    "ax.plot(siamese_c_val_losses, label='Val Loss', color='red')\n",
    "ax.set_xlabel('√âpoca')\n",
    "ax.set_ylabel('Contrastive Loss')\n",
    "ax.set_title('Red Siamesa (Contrastive Loss) - Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb36e7",
   "metadata": {},
   "source": [
    "### 7.1 Clasificaci√≥n con FC Layer (Contrastive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06543fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificador basado en embeddings de la red siamesa\n",
    "class SiameseClassifier(nn.Module):\n",
    "    \"\"\"Clasificador que usa embeddings de red siamesa\"\"\"\n",
    "    def __init__(self, siamese_network, embedding_dim, num_classes, freeze_backbone=True):\n",
    "        super(SiameseClassifier, self).__init__()\n",
    "        \n",
    "        self.siamese = siamese_network\n",
    "        \n",
    "        # Congelar backbone si se especifica\n",
    "        if freeze_backbone:\n",
    "            for param in self.siamese.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Clasificador\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Obtener embedding\n",
    "        with torch.no_grad():\n",
    "            embedding = self.siamese.forward_one(x)\n",
    "        # Clasificar\n",
    "        return self.classifier(embedding)\n",
    "\n",
    "# Crear clasificador con embeddings contrastive\n",
    "siamese_contrastive_fc = SiameseClassifier(\n",
    "    siamese_contrastive, config.EMBEDDING_DIM, config.NUM_CLASSES, freeze_backbone=True\n",
    ").to(device)\n",
    "\n",
    "print(\"üèóÔ∏è Clasificador Siamesa (Contrastive) + FC:\")\n",
    "print(f\"   ‚Ä¢ Par√°metros entrenables: {sum(p.numel() for p in siamese_contrastive_fc.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf643dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar clasificador FC sobre embeddings contrastive\n",
    "siamese_contrastive_fc, sc_fc_train_losses, sc_fc_val_losses, sc_fc_train_accs, sc_fc_val_accs = train_cnn_model(\n",
    "    siamese_contrastive_fc, train_loader, val_loader, \n",
    "    num_epochs=30, lr=1e-3, device=device,\n",
    "    model_name=\"Siamesa Contrastive + FC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2627e497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar y evaluar clasificador contrastive + FC\n",
    "plot_training_history(sc_fc_train_losses, sc_fc_val_losses, sc_fc_train_accs, sc_fc_val_accs, \n",
    "                      title=\"Siamesa Contrastive + FC\")\n",
    "\n",
    "print(\"üìä Evaluaci√≥n de Siamesa Contrastive + FC en conjunto de Test:\")\n",
    "sc_fc_labels, sc_fc_preds = evaluate_model(siamese_contrastive_fc, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507982e1",
   "metadata": {},
   "source": [
    "### 7.2 Clasificaci√≥n con XGBoost (Contrastive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148886a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funci√≥n para extraer embeddings\n",
    "def extract_embeddings(siamese_model, data_loader, device):\n",
    "    \"\"\"Extrae embeddings de todas las im√°genes usando la red siamesa\"\"\"\n",
    "    siamese_model.eval()\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, lbls in tqdm(data_loader, desc=\"Extracting embeddings\"):\n",
    "            images = images.to(device)\n",
    "            emb = siamese_model.forward_one(images)\n",
    "            embeddings.append(emb.cpu().numpy())\n",
    "            labels.extend(lbls.numpy())\n",
    "    \n",
    "    return np.vstack(embeddings), np.array(labels)\n",
    "\n",
    "# Extraer embeddings usando la red siamesa con contrastive loss\n",
    "print(\"üîÑ Extrayendo embeddings (Contrastive)...\")\n",
    "train_embeddings_c, train_labels_c = extract_embeddings(siamese_contrastive, train_loader, device)\n",
    "test_embeddings_c, test_labels_c = extract_embeddings(siamese_contrastive, test_loader, device)\n",
    "\n",
    "print(f\"‚úÖ Embeddings extra√≠dos:\")\n",
    "print(f\"   ‚Ä¢ Train: {train_embeddings_c.shape}\")\n",
    "print(f\"   ‚Ä¢ Test: {test_embeddings_c.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d604aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar XGBoost con embeddings contrastive\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"üöÄ Entrenando XGBoost con embeddings Contrastive...\")\n",
    "    \n",
    "    xgb_contrastive = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    xgb_contrastive.fit(train_embeddings_c, train_labels_c)\n",
    "    \n",
    "    # Evaluar\n",
    "    xgb_c_preds = xgb_contrastive.predict(test_embeddings_c)\n",
    "    xgb_c_accuracy = accuracy_score(test_labels_c, xgb_c_preds)\n",
    "    \n",
    "    print(f\"\\nüìä XGBoost (Contrastive) - Test Accuracy: {xgb_c_accuracy*100:.2f}%\")\n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(test_labels_c, xgb_c_preds, target_names=class_names))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è XGBoost no disponible. Instalar con: pip install xgboost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3590da",
   "metadata": {},
   "source": [
    "## 8. Red Siamesa + Triplet Loss\n",
    "\n",
    "Implementaci√≥n de red siamesa con Triplet Loss:\n",
    "- Tripletes: Anchor, Positive (misma clase), Negative (diferente clase)\n",
    "- Margin para separaci√≥n en el espacio de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b9d309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset para tripletes\n",
    "class TripletDataset(Dataset):\n",
    "    \"\"\"Dataset que genera tripletes para Triplet Loss\"\"\"\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Organizar im√°genes por clase\n",
    "        self.class_to_indices = {}\n",
    "        for idx, label in enumerate(labels):\n",
    "            if label not in self.class_to_indices:\n",
    "                self.class_to_indices[label] = []\n",
    "            self.class_to_indices[label].append(idx)\n",
    "        \n",
    "        self.classes = list(self.class_to_indices.keys())\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Anchor\n",
    "        anchor_path = self.image_paths[idx]\n",
    "        anchor_label = self.labels[idx]\n",
    "        \n",
    "        # Positive (misma clase que anchor, diferente imagen)\n",
    "        positive_indices = [i for i in self.class_to_indices[anchor_label] if i != idx]\n",
    "        if len(positive_indices) == 0:\n",
    "            positive_idx = idx\n",
    "        else:\n",
    "            positive_idx = random.choice(positive_indices)\n",
    "        positive_path = self.image_paths[positive_idx]\n",
    "        \n",
    "        # Negative (diferente clase)\n",
    "        negative_classes = [c for c in self.classes if c != anchor_label]\n",
    "        negative_class = random.choice(negative_classes)\n",
    "        negative_idx = random.choice(self.class_to_indices[negative_class])\n",
    "        negative_path = self.image_paths[negative_idx]\n",
    "        \n",
    "        # Cargar im√°genes\n",
    "        anchor = Image.open(anchor_path).convert('RGB')\n",
    "        positive = Image.open(positive_path).convert('RGB')\n",
    "        negative = Image.open(negative_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            anchor = self.transform(anchor)\n",
    "            positive = self.transform(positive)\n",
    "            negative = self.transform(negative)\n",
    "        \n",
    "        return anchor, positive, negative\n",
    "\n",
    "# Crear datasets de tripletes\n",
    "triplet_train_dataset = TripletDataset(X_train, y_train, transform=train_transforms)\n",
    "triplet_val_dataset = TripletDataset(X_val, y_val, transform=val_test_transforms)\n",
    "\n",
    "triplet_train_loader = DataLoader(triplet_train_dataset, batch_size=config.BATCH_SIZE, \n",
    "                                   shuffle=True, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "triplet_val_loader = DataLoader(triplet_val_dataset, batch_size=config.BATCH_SIZE, \n",
    "                                 shuffle=False, num_workers=config.NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(f\"‚úÖ Datasets de tripletes creados\")\n",
    "print(f\"   ‚Ä¢ Train triplets: {len(triplet_train_dataset)}\")\n",
    "print(f\"   ‚Ä¢ Val triplets: {len(triplet_val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5627aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Triplet Loss\n",
    "class TripletLoss(nn.Module):\n",
    "    \"\"\"Triplet Loss con margin\"\"\"\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        # Distancias\n",
    "        distance_positive = F.pairwise_distance(anchor, positive)\n",
    "        distance_negative = F.pairwise_distance(anchor, negative)\n",
    "        \n",
    "        # Triplet loss\n",
    "        losses = F.relu(distance_positive - distance_negative + self.margin)\n",
    "        return losses.mean()\n",
    "\n",
    "# Funciones de entrenamiento para triplet loss\n",
    "def train_triplet_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Entrena la red siamesa con triplet loss por una √©poca\"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=\"Training Triplet\", leave=False)\n",
    "    for anchor, positive, negative in pbar:\n",
    "        anchor = anchor.to(device)\n",
    "        positive = positive.to(device)\n",
    "        negative = negative.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        emb_anchor = model.forward_one(anchor)\n",
    "        emb_positive = model.forward_one(positive)\n",
    "        emb_negative = model.forward_one(negative)\n",
    "        \n",
    "        loss = criterion(emb_anchor, emb_positive, emb_negative)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "def validate_triplet(model, val_loader, criterion, device):\n",
    "    \"\"\"Valida la red siamesa con triplet loss\"\"\"\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for anchor, positive, negative in val_loader:\n",
    "            anchor = anchor.to(device)\n",
    "            positive = positive.to(device)\n",
    "            negative = negative.to(device)\n",
    "            \n",
    "            emb_anchor = model.forward_one(anchor)\n",
    "            emb_positive = model.forward_one(positive)\n",
    "            emb_negative = model.forward_one(negative)\n",
    "            \n",
    "            loss = criterion(emb_anchor, emb_positive, emb_negative)\n",
    "            running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(val_loader)\n",
    "\n",
    "def train_triplet_model(model, train_loader, val_loader, criterion, num_epochs, lr, device, model_name=\"Siamese Triplet\"):\n",
    "    \"\"\"Funci√≥n completa de entrenamiento de red siamesa con triplet loss\"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(f\"\\nüöÄ Iniciando entrenamiento de {model_name}\")\n",
    "    print(f\"   ‚Ä¢ √âpocas: {num_epochs}\")\n",
    "    print(f\"   ‚Ä¢ Learning Rate: {lr}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_triplet_epoch(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = validate_triplet(model, val_loader, criterion, device)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        scheduler.step()\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    model.load_state_dict(best_model_state)\n",
    "    print(f\"\\n‚úÖ Mejor Val Loss: {best_val_loss:.4f}\")\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "print(\"‚úÖ Triplet Loss y funciones de entrenamiento definidas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaa6c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear y entrenar red siamesa con Triplet Loss\n",
    "siamese_triplet = SiameseNetwork(embedding_dim=config.EMBEDDING_DIM).to(device)\n",
    "triplet_loss_fn = TripletLoss(margin=1.0)\n",
    "\n",
    "print(\"üèóÔ∏è Red Siamesa con Triplet Loss:\")\n",
    "print(f\"   ‚Ä¢ Par√°metros totales: {sum(p.numel() for p in siamese_triplet.parameters()):,}\")\n",
    "\n",
    "# Entrenar\n",
    "siamese_triplet, siamese_t_train_losses, siamese_t_val_losses = train_triplet_model(\n",
    "    siamese_triplet, triplet_train_loader, triplet_val_loader, \n",
    "    triplet_loss_fn, num_epochs=config.NUM_EPOCHS_SIAMESE, \n",
    "    lr=config.LR_SIAMESE, device=device,\n",
    "    model_name=\"Red Siamesa (Triplet Loss)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2fd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar p√©rdida de entrenamiento triplet\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(siamese_t_train_losses, label='Train Loss', color='blue')\n",
    "ax.plot(siamese_t_val_losses, label='Val Loss', color='red')\n",
    "ax.set_xlabel('√âpoca')\n",
    "ax.set_ylabel('Triplet Loss')\n",
    "ax.set_title('Red Siamesa (Triplet Loss) - Training History')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d017709",
   "metadata": {},
   "source": [
    "### 8.1 Clasificaci√≥n con FC Layer (Triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1946d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clasificador con embeddings triplet + FC\n",
    "siamese_triplet_fc = SiameseClassifier(\n",
    "    siamese_triplet, config.EMBEDDING_DIM, config.NUM_CLASSES, freeze_backbone=True\n",
    ").to(device)\n",
    "\n",
    "print(\"üèóÔ∏è Clasificador Siamesa (Triplet) + FC:\")\n",
    "print(f\"   ‚Ä¢ Par√°metros entrenables: {sum(p.numel() for p in siamese_triplet_fc.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# Entrenar\n",
    "siamese_triplet_fc, st_fc_train_losses, st_fc_val_losses, st_fc_train_accs, st_fc_val_accs = train_cnn_model(\n",
    "    siamese_triplet_fc, train_loader, val_loader, \n",
    "    num_epochs=30, lr=1e-3, device=device,\n",
    "    model_name=\"Siamesa Triplet + FC\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ae043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar y evaluar clasificador triplet + FC\n",
    "plot_training_history(st_fc_train_losses, st_fc_val_losses, st_fc_train_accs, st_fc_val_accs, \n",
    "                      title=\"Siamesa Triplet + FC\")\n",
    "\n",
    "print(\"üìä Evaluaci√≥n de Siamesa Triplet + FC en conjunto de Test:\")\n",
    "st_fc_labels, st_fc_preds = evaluate_model(siamese_triplet_fc, test_loader, device, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d530163b",
   "metadata": {},
   "source": [
    "### 8.2 Clasificaci√≥n con XGBoost (Triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60159fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer embeddings usando la red siamesa con triplet loss\n",
    "print(\"üîÑ Extrayendo embeddings (Triplet)...\")\n",
    "train_embeddings_t, train_labels_t = extract_embeddings(siamese_triplet, train_loader, device)\n",
    "test_embeddings_t, test_labels_t = extract_embeddings(siamese_triplet, test_loader, device)\n",
    "\n",
    "print(f\"‚úÖ Embeddings extra√≠dos:\")\n",
    "print(f\"   ‚Ä¢ Train: {train_embeddings_t.shape}\")\n",
    "print(f\"   ‚Ä¢ Test: {test_embeddings_t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar XGBoost con embeddings triplet\n",
    "if XGBOOST_AVAILABLE:\n",
    "    print(\"üöÄ Entrenando XGBoost con embeddings Triplet...\")\n",
    "    \n",
    "    xgb_triplet = xgb.XGBClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        use_label_encoder=False,\n",
    "        eval_metric='mlogloss',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    xgb_triplet.fit(train_embeddings_t, train_labels_t)\n",
    "    \n",
    "    # Evaluar\n",
    "    xgb_t_preds = xgb_triplet.predict(test_embeddings_t)\n",
    "    xgb_t_accuracy = accuracy_score(test_labels_t, xgb_t_preds)\n",
    "    \n",
    "    print(f\"\\nüìä XGBoost (Triplet) - Test Accuracy: {xgb_t_accuracy*100:.2f}%\")\n",
    "    print(\"\\nüìã Classification Report:\")\n",
    "    print(classification_report(test_labels_t, xgb_t_preds, target_names=class_names))\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è XGBoost no disponible\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a36ea60",
   "metadata": {},
   "source": [
    "## 9. Buscador de Similaridad - Top 10 Im√°genes Similares\n",
    "\n",
    "Sistema de b√∫squeda de im√°genes similares usando los embeddings de la red siamesa:\n",
    "- Utiliza K-Nearest Neighbors en el espacio de embeddings\n",
    "- Retorna las 10 im√°genes m√°s similares a una imagen de consulta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6374e0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para b√∫squeda de similaridad\n",
    "class SimilaritySearcher:\n",
    "    \"\"\"Buscador de im√°genes similares usando embeddings\"\"\"\n",
    "    def __init__(self, siamese_model, image_paths, labels, transform, device, n_neighbors=10):\n",
    "        self.siamese_model = siamese_model\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "        self.n_neighbors = n_neighbors\n",
    "        \n",
    "        # Extraer embeddings de todas las im√°genes\n",
    "        print(\"üîÑ Construyendo √≠ndice de embeddings...\")\n",
    "        self.embeddings = self._extract_all_embeddings()\n",
    "        \n",
    "        # Crear √≠ndice KNN\n",
    "        self.knn = NearestNeighbors(n_neighbors=n_neighbors, metric='euclidean')\n",
    "        self.knn.fit(self.embeddings)\n",
    "        print(f\"‚úÖ √çndice creado con {len(self.embeddings)} im√°genes\")\n",
    "    \n",
    "    def _extract_all_embeddings(self):\n",
    "        \"\"\"Extrae embeddings de todas las im√°genes\"\"\"\n",
    "        self.siamese_model.eval()\n",
    "        embeddings = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for img_path in tqdm(self.image_paths, desc=\"Indexing\"):\n",
    "                img = Image.open(img_path).convert('RGB')\n",
    "                img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "                emb = self.siamese_model.forward_one(img_tensor)\n",
    "                embeddings.append(emb.cpu().numpy().flatten())\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def search(self, query_image_path, k=10):\n",
    "        \"\"\"Busca las k im√°genes m√°s similares a una imagen de consulta\"\"\"\n",
    "        # Obtener embedding de la imagen de consulta\n",
    "        self.siamese_model.eval()\n",
    "        with torch.no_grad():\n",
    "            img = Image.open(query_image_path).convert('RGB')\n",
    "            img_tensor = self.transform(img).unsqueeze(0).to(self.device)\n",
    "            query_embedding = self.siamese_model.forward_one(img_tensor).cpu().numpy()\n",
    "        \n",
    "        # Buscar vecinos m√°s cercanos\n",
    "        distances, indices = self.knn.kneighbors(query_embedding, n_neighbors=k)\n",
    "        \n",
    "        results = []\n",
    "        for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
    "            results.append({\n",
    "                'rank': i + 1,\n",
    "                'path': self.image_paths[idx],\n",
    "                'label': self.labels[idx],\n",
    "                'distance': dist\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_search(self, query_image_path, k=10):\n",
    "        \"\"\"Visualiza los resultados de b√∫squeda\"\"\"\n",
    "        results = self.search(query_image_path, k)\n",
    "        query_label = None\n",
    "        \n",
    "        # Encontrar etiqueta de la imagen de consulta\n",
    "        if query_image_path in self.image_paths:\n",
    "            idx = self.image_paths.index(query_image_path)\n",
    "            query_label = self.labels[idx]\n",
    "        \n",
    "        # Visualizar\n",
    "        fig, axes = plt.subplots(2, k//2 + 1, figsize=(20, 8))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        # Imagen de consulta\n",
    "        query_img = Image.open(query_image_path)\n",
    "        axes[0].imshow(query_img)\n",
    "        axes[0].set_title(f'üîç QUERY\\n{query_label}', fontsize=10, fontweight='bold', color='blue')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Resultados\n",
    "        for i, result in enumerate(results):\n",
    "            ax = axes[i + 1]\n",
    "            img = Image.open(result['path'])\n",
    "            ax.imshow(img)\n",
    "            \n",
    "            # Color verde si es la misma clase, rojo si es diferente\n",
    "            color = 'green' if result['label'] == query_label else 'red'\n",
    "            ax.set_title(f\"#{result['rank']}: {result['label'][:12]}\\nDist: {result['distance']:.3f}\", \n",
    "                        fontsize=8, color=color)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Ocultar ejes vac√≠os\n",
    "        for i in range(len(results) + 1, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.suptitle('üîç B√∫squeda de Im√°genes Similares - Top 10', fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Estad√≠sticas\n",
    "        same_class_count = sum(1 for r in results if r['label'] == query_label)\n",
    "        print(f\"\\nüìä Precisi√≥n: {same_class_count}/{k} im√°genes de la misma clase ({same_class_count/k*100:.1f}%)\")\n",
    "\n",
    "print(\"‚úÖ Clase SimilaritySearcher definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e3d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear buscador de similaridad usando los embeddings de la red triplet (mejor para similaridad)\n",
    "similarity_searcher = SimilaritySearcher(\n",
    "    siamese_model=siamese_triplet,\n",
    "    image_paths=all_images,\n",
    "    labels=all_labels,\n",
    "    transform=val_test_transforms,\n",
    "    device=device,\n",
    "    n_neighbors=11  # 10 + 1 para excluir la propia imagen si est√° en el √≠ndice\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bab9bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostraci√≥n de b√∫squeda de similaridad con varias im√°genes de diferentes clases\n",
    "print(\"üîç Demostraci√≥n del Buscador de Similaridad\\n\")\n",
    "\n",
    "# Seleccionar im√°genes de consulta aleatorias de diferentes clases\n",
    "demo_classes = random.sample(list(set(all_labels)), 5)\n",
    "\n",
    "for class_name in demo_classes:\n",
    "    # Obtener una imagen de esta clase\n",
    "    class_images = [img for img, lbl in zip(all_images, all_labels) if lbl == class_name]\n",
    "    query_image = random.choice(class_images)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üçé Clase de consulta: {class_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    similarity_searcher.visualize_search(query_image, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbe12ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n del buscador de similaridad\n",
    "def evaluate_similarity_search(searcher, test_images, test_labels, k=10):\n",
    "    \"\"\"Eval√∫a la precisi√≥n del buscador de similaridad\"\"\"\n",
    "    total_precision = 0\n",
    "    n_queries = min(100, len(test_images))  # Evaluar sobre 100 im√°genes aleatorias\n",
    "    \n",
    "    sample_indices = random.sample(range(len(test_images)), n_queries)\n",
    "    \n",
    "    for idx in tqdm(sample_indices, desc=\"Evaluating similarity search\"):\n",
    "        query_path = test_images[idx]\n",
    "        query_label = test_labels[idx]\n",
    "        \n",
    "        results = searcher.search(query_path, k=k)\n",
    "        \n",
    "        # Excluir la propia imagen si aparece en los resultados\n",
    "        results = [r for r in results if r['path'] != query_path][:k]\n",
    "        \n",
    "        # Calcular precisi√≥n\n",
    "        same_class_count = sum(1 for r in results if r['label'] == query_label)\n",
    "        precision = same_class_count / len(results) if results else 0\n",
    "        total_precision += precision\n",
    "    \n",
    "    avg_precision = total_precision / n_queries\n",
    "    return avg_precision\n",
    "\n",
    "# Evaluar\n",
    "print(\"üìä Evaluaci√≥n del Buscador de Similaridad en Test Set:\")\n",
    "precision_at_10 = evaluate_similarity_search(similarity_searcher, X_test, y_test, k=10)\n",
    "print(f\"\\n‚úÖ Precision@10: {precision_at_10*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365fae20",
   "metadata": {},
   "source": [
    "## 10. Resumen y Comparaci√≥n de Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38463cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de resultados\n",
    "results_summary = []\n",
    "\n",
    "# CNN desde cero\n",
    "cnn_accuracy = accuracy_score(cnn_labels, cnn_preds)\n",
    "results_summary.append({\n",
    "    'Modelo': 'CNN desde cero',\n",
    "    'Accuracy (%)': f'{cnn_accuracy*100:.2f}',\n",
    "    'Tipo': 'Clasificaci√≥n directa'\n",
    "})\n",
    "\n",
    "# Swin Transformer\n",
    "swin_accuracy = accuracy_score(swin_labels, swin_preds)\n",
    "results_summary.append({\n",
    "    'Modelo': 'Swin Transformer',\n",
    "    'Accuracy (%)': f'{swin_accuracy*100:.2f}',\n",
    "    'Tipo': 'Transfer Learning'\n",
    "})\n",
    "\n",
    "# Siamesa Contrastive + FC\n",
    "sc_fc_accuracy = accuracy_score(sc_fc_labels, sc_fc_preds)\n",
    "results_summary.append({\n",
    "    'Modelo': 'Siamesa Contrastive + FC',\n",
    "    'Accuracy (%)': f'{sc_fc_accuracy*100:.2f}',\n",
    "    'Tipo': 'Embedding + FC'\n",
    "})\n",
    "\n",
    "# Siamesa Contrastive + XGBoost\n",
    "if XGBOOST_AVAILABLE:\n",
    "    results_summary.append({\n",
    "        'Modelo': 'Siamesa Contrastive + XGBoost',\n",
    "        'Accuracy (%)': f'{xgb_c_accuracy*100:.2f}',\n",
    "        'Tipo': 'Embedding + XGBoost'\n",
    "    })\n",
    "\n",
    "# Siamesa Triplet + FC\n",
    "st_fc_accuracy = accuracy_score(st_fc_labels, st_fc_preds)\n",
    "results_summary.append({\n",
    "    'Modelo': 'Siamesa Triplet + FC',\n",
    "    'Accuracy (%)': f'{st_fc_accuracy*100:.2f}',\n",
    "    'Tipo': 'Embedding + FC'\n",
    "})\n",
    "\n",
    "# Siamesa Triplet + XGBoost\n",
    "if XGBOOST_AVAILABLE:\n",
    "    results_summary.append({\n",
    "        'Modelo': 'Siamesa Triplet + XGBoost',\n",
    "        'Accuracy (%)': f'{xgb_t_accuracy*100:.2f}',\n",
    "        'Tipo': 'Embedding + XGBoost'\n",
    "    })\n",
    "\n",
    "# Crear DataFrame\n",
    "df_results = pd.DataFrame(results_summary)\n",
    "print(\"üìä RESUMEN DE RESULTADOS - FruitSeg30\")\n",
    "print(\"=\" * 60)\n",
    "print(df_results.to_string(index=False))\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa2e616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizaci√≥n comparativa\n",
    "accuracies = [float(r['Accuracy (%)']) for r in results_summary]\n",
    "models = [r['Modelo'] for r in results_summary]\n",
    "colors = plt.cm.viridis(np.linspace(0, 0.8, len(models)))\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = plt.barh(models, accuracies, color=colors)\n",
    "plt.xlabel('Accuracy (%)')\n",
    "plt.title('üìä Comparaci√≥n de Modelos - FruitSeg30')\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "# A√±adir etiquetas de valor\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    plt.text(acc + 1, bar.get_y() + bar.get_height()/2, f'{acc:.2f}%', \n",
    "             va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Mejor modelo\n",
    "best_idx = np.argmax(accuracies)\n",
    "print(f\"\\nüèÜ Mejor modelo: {models[best_idx]} con {accuracies[best_idx]:.2f}% accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403dee41",
   "metadata": {},
   "source": [
    "## 11. Conclusiones\n",
    "\n",
    "### Resumen de Implementaciones:\n",
    "\n",
    "1. **CNN desde cero**: Arquitectura VGG-like con 5 bloques convolucionales\n",
    "2. **Swin Transformer**: Transfer learning con fine-tuning completo\n",
    "3. **Red Siamesa + Contrastive Loss**: Aprendizaje de embeddings con pares\n",
    "4. **Red Siamesa + Triplet Loss**: Aprendizaje de embeddings con tripletes\n",
    "5. **Clasificaci√≥n con XGBoost**: Usando embeddings como features\n",
    "6. **Buscador de Similaridad**: Top-10 im√°genes similares usando KNN\n",
    "\n",
    "### Observaciones:\n",
    "- Los modelos con transfer learning (Swin) suelen obtener mejores resultados\n",
    "- Los embeddings de redes siamesas son √∫tiles tanto para clasificaci√≥n como para b√∫squeda\n",
    "- La Triplet Loss generalmente produce mejores embeddings para tareas de similaridad\n",
    "- XGBoost puede ser una alternativa eficiente sobre embeddings precomputados"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
